{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data Importing and Cleaning"
      ],
      "metadata": {
        "id": "wRn4tTVaQDdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Installing and Downloading Packages*"
      ],
      "metadata": {
        "id": "rEivO-5uQ81G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFW6i3_VYYlj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Mounting Google Drive*"
      ],
      "metadata": {
        "id": "whF5gfCCRHp9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5RzAc2iymkz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####*Importing the Dataset file*"
      ],
      "metadata": {
        "id": "68UOEwiXRPN-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDGYbFyraKdJ"
      },
      "outputs": [],
      "source": [
        "csv_path = '/content/drive/MyDrive/Colab Notebooks/Sentiment_analysis/Dataset/training.1600000.processed.noemoticon.csv'\n",
        "encodings = ['utf-8', 'ISO-8859-1', 'cp1252']\n",
        "df = None\n",
        "for encoding in encodings:\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path, encoding=encoding)\n",
        "        break\n",
        "    except UnicodeDecodeError:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Giving Column Names*"
      ],
      "metadata": {
        "id": "RJ_d3pSRRZEM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-M0XzGYdX-w"
      },
      "outputs": [],
      "source": [
        "new_column_names = ['score', 'id' ,'Date', 'Query' , 'username' , 'text']\n",
        "df.columns = new_column_names"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## --> Data Transformation"
      ],
      "metadata": {
        "id": "M_ZBifzRRexu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *Cleaning of text using Lowercasing , removal of Unwanted characters , Tokenization , Stop Word Removal , Lemmatization*"
      ],
      "metadata": {
        "id": "ktnUyvmzRqCn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8Y08qK8d3PT"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Removing special characters and punctuation\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Stopword removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "    # Join lemmatized tokens back into a cleaned text\n",
        "    cleaned_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Apply data preprocessing to the 'text' column of the DataFrame\n",
        "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Print a sample of the cleaned text\n",
        "print(df['cleaned_text'].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_5yOf18ZFUR"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## --> Installing Required modules"
      ],
      "metadata": {
        "id": "6HUKGOnOSEdO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WX_ZDF3iZHqY"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt8Cfw0DZJgD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## --> Implementing BERT model"
      ],
      "metadata": {
        "id": "cOIIBVydSUur"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX8KuoC7ZLJu"
      },
      "outputs": [],
      "source": [
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "num_labels = 3  # Three classes: positive, negative, neutral\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####--> Classifying as Positive Negative and Neutral"
      ],
      "metadata": {
        "id": "B9MTFAhMS7m8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFR-LUtjZMoZ"
      },
      "outputs": [],
      "source": [
        "class_labels = ['negative', 'neutral', 'positive']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHMJW7QHZPYn"
      },
      "outputs": [],
      "source": [
        "cleaned_texts = df['cleaned_text'].tolist()\n",
        "scores = df['score'].tolist()\n",
        "encoded_texts = tokenizer(cleaned_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "input_ids = encoded_texts['input_ids']\n",
        "attention_mask = encoded_texts['attention_mask']\n",
        "labels = torch.tensor(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###--> Splitting Train and Test Dataset"
      ],
      "metadata": {
        "id": "9JTrG3heSdsb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isDCyYOkeNmr"
      },
      "outputs": [],
      "source": [
        "train_data, test_data, train_labels, test_labels = train_test_split(input_ids, labels, test_size=0.2, random_state=42)\n",
        "val_data, test_data, val_labels, test_labels = train_test_split(test_data, test_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "batch_size = 16\n",
        "train_dataset = TensorDataset(train_data, train_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = TensorDataset(val_data, val_labels)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "test_dataset = TensorDataset(test_data, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### --> Optimization"
      ],
      "metadata": {
        "id": "FQR-FNnaTh_G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r49VgUSePyW"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## --> Training the model"
      ],
      "metadata": {
        "id": "-_bQ7lKPTpe5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1vFzwlseSCX"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    # ... training loop\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    # ... validation loop"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## -->Model Evalutation with Accuracy , Classification Report and Confusion Matrix"
      ],
      "metadata": {
        "id": "XK8ic-0GT0G6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT7Xh3G1eUhe"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "test_predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids, labels = batch\n",
        "        outputs = model(input_ids)\n",
        "        predicted = torch.argmax(outputs.logits, dim=1)\n",
        "        test_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy\n",
        "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "# Classification report\n",
        "target_names = ['negative', 'neutral', 'positive']\n",
        "print(classification_report(test_true_labels, test_predictions, target_names=target_names))\n",
        "\n",
        "# Confusion matrix\n",
        "confusion = confusion_matrix(test_true_labels, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}